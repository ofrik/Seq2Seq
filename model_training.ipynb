{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ofri\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:862: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import seed\n",
    "\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "\n",
    "set_random_seed(2)\n",
    "from util import read_data,read_data2, get_vocab, vectorize_sentences, indent_sentences\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.seq2seq as seq2seq\n",
    "from tensorflow.python.layers.core import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SENTENCE_START_TOKEN = \"<START>\"\n",
    "SENTENCE_END_TOKEN = \"<EOS>\"\n",
    "OOV_TOKEN = \"<UNK>\"\n",
    "PAD_TOKEN = \"<PAD>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = read_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df = df[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "msk = np.random.rand(len(df)) < 0.8\n",
    "df_train = df[msk]\n",
    "df_test = df[~msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 128719\n",
      "Test size: 32232\n"
     ]
    }
   ],
   "source": [
    "print(\"Train size: %s\"%len(df_train))\n",
    "print(\"Test size: %s\"%len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 160951/160951 [00:16<00:00, 9675.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original vocab size 20550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 160951/160951 [00:16<00:00, 9885.04it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original vocab size 55736\n"
     ]
    }
   ],
   "source": [
    "eng_vocab, rev_eng_vocab = get_vocab(df[\"english_sentences\"], addtional_tokens=[PAD_TOKEN,OOV_TOKEN], top=15000)\n",
    "heb_vocab, rev_heb_vocab = get_vocab(df[\"hebrew_sentences\"],\n",
    "                                         addtional_tokens=[PAD_TOKEN,OOV_TOKEN, SENTENCE_START_TOKEN, SENTENCE_END_TOKEN],\n",
    "                                         top=30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128719/128719 [00:12<00:00, 10300.76it/s]\n",
      "100%|██████████| 128719/128719 [00:11<00:00, 10803.61it/s]\n"
     ]
    }
   ],
   "source": [
    "vect_eng_sentences = vectorize_sentences(df_train[\"english_sentences\"], eng_vocab,encode=True,reverse=True)\n",
    "decoder_input_data = vectorize_sentences(df_train[\"hebrew_sentences\"], heb_vocab, add_prefix_token=SENTENCE_START_TOKEN,encode=True)\n",
    "decoder_target_data = np.array([np.concatenate((x[1:], [heb_vocab[SENTENCE_END_TOKEN]]), axis=0) for x in decoder_input_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size_english = len(eng_vocab)\n",
    "vocab_size_hebrew = len(heb_vocab)\n",
    "max_encoder_seq_length = max([len(txt) for txt in vect_eng_sentences])\n",
    "max_decoder_seq_length = max([len(txt) for txt in decoder_input_data])\n",
    "emb_dim = 100\n",
    "lstm_size = 128\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pad_sequences(sequence,pad_id,to_length):\n",
    "    need_to_add = to_length-len(sequence)\n",
    "    return np.concatenate((sequence,np.array([pad_id]*need_to_add)),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_eng_sentences = np.array([pad_sequences(sentence,eng_vocab[PAD_TOKEN],max_encoder_seq_length) for sentence in vect_eng_sentences])\n",
    "decoder_input_data = np.array([pad_sequences(sentence,heb_vocab[PAD_TOKEN],max_decoder_seq_length) for sentence in decoder_input_data])\n",
    "decoder_target_data = np.array([pad_sequences(sentence,heb_vocab[PAD_TOKEN],max_decoder_seq_length) for sentence in decoder_target_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inputs and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_inputs = tf.placeholder(shape=(None, None), dtype=tf.int32, name='encoder_inputs')\n",
    "decoder_inputs = tf.placeholder(shape=(None, None), dtype=tf.int32, name='decoder_inputs')\n",
    "decoder_targets = tf.placeholder(shape=(None, None), dtype=tf.int32, name='decoder_targets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_inputs_length = tf.placeholder(shape=(None,), dtype=tf.int32, name='encoder_inputs_length')\n",
    "decoder_inputs_length = tf.placeholder(shape=(None,), dtype=tf.int32, name='decoder_inputs_length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = tf.placeholder(tf.float32, shape=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_encoder = tf.get_variable(\"embedding_encoder\", [vocab_size_english, emb_dim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_emb_inp = tf.nn.embedding_lookup(embedding_encoder, encoder_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_decoder = tf.get_variable(\"embedding_decoder\", [vocab_size_hebrew, emb_dim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder_emb_inp = tf.nn.embedding_lookup(embedding_decoder, decoder_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_initializer = tf.random_uniform_initializer(-0.08,0.08)\n",
    "# lstm_initializer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_cell = tf.contrib.rnn.MultiRNNCell([tf.nn.rnn_cell.LSTMCell(lstm_size,initializer=lstm_initializer) for _ in range(2)])\n",
    "# encoder_cell = tf.nn.rnn_cell.LSTMCell(lstm_size,initializer=lstm_initializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_outputs, encoder_state = tf.nn.dynamic_rnn(\n",
    "    encoder_cell, encoder_emb_inp,sequence_length=encoder_inputs_length,dtype=tf.float32, time_major=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder_cell = tf.contrib.rnn.MultiRNNCell([tf.nn.rnn_cell.LSTMCell(lstm_size,initializer=lstm_initializer) for _ in range(2)])\n",
    "# decoder_cell = tf.nn.rnn_cell.LSTMCell(lstm_size,initializer=lstm_initializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_outputs, decoder_final_state = tf.nn.dynamic_rnn(\n",
    "    decoder_cell, decoder_emb_inp,initial_state=encoder_state,sequence_length=decoder_inputs_length,\n",
    "    dtype=tf.float32, time_major=True, scope=\"plain_decoder\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder_logits = tf.contrib.layers.linear(decoder_outputs, vocab_size_hebrew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder_prediction = tf.argmax(decoder_logits, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "stepwise_cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=tf.one_hot(decoder_targets, depth=vocab_size_hebrew, dtype=tf.float32),\n",
    "                                                                 logits=decoder_logits)\n",
    "\n",
    "loss = tf.reduce_mean(stepwise_cross_entropy)\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "gvs = optimizer.compute_gradients(loss)\n",
    "capped_gvs = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gvs]\n",
    "train_op = optimizer.apply_gradients(capped_gvs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(batch_size):\n",
    "    number_of_batches = int(len(vect_eng_sentences)/batch_size)\n",
    "    counter = 0\n",
    "    while True:\n",
    "        from_index = (counter%number_of_batches)*batch_size\n",
    "        to_index = ((counter%number_of_batches)+1)*(batch_size)\n",
    "        yield {encoder_inputs:vect_eng_sentences[from_index:to_index].T,\n",
    "              decoder_inputs:decoder_input_data[from_index:to_index].T,\n",
    "              decoder_targets:decoder_target_data[from_index:to_index].T,\n",
    "              encoder_inputs_length:np.array([len([word for word in sequence if word!=0]) for sequence in vect_eng_sentences[from_index:to_index]],dtype=np.int32),\n",
    "              decoder_inputs_length:np.array([len([word for word in sequence if word!=0]) for sequence in decoder_input_data[from_index:to_index]],dtype=np.int32)}\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decode_sequence(sequence,rev_vocab):\n",
    "    return \" \".join([rev_vocab[int(index)] for index in sequence if rev_vocab[int(index)]!=\"<PAD>\" and rev_vocab[int(index)]!=\"<EOS>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_track = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "  minibatch loss: 10.308975219726562\n",
      "  sample 1:\n",
      "    input     > let is try something .\n",
      "    predicted_raw > [3 3 3 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    expected_raw > [   94.  1243.    73.    23.     3.     0.     0.     0.     0.     0.\n",
      "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "     0.     0.     0.     0.     0.]\n",
      "    predicted > \n",
      "    expected > בוא ננסה משהו !\n",
      "  sample 2:\n",
      "    input     > i have to go to sleep .\n",
      "    predicted_raw > [10  3  3  3  3  3  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "    expected_raw > [   6.   38.   85.  317.    4.    3.    0.    0.    0.    0.    0.    0.\n",
      "    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "    0.    0.    0.    0.    0.]\n",
      "    predicted > זה\n",
      "    expected > אני צריך ללכת לישון .\n",
      "  sample 3:\n",
      "    input     > i have to go to sleep .\n",
      "    predicted_raw > [10  3  3  3  3  3  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "    expected_raw > [    6.  1796.    85.   317.     4.     3.     0.     0.     0.     0.\n",
      "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "     0.     0.     0.     0.     0.]\n",
      "    predicted > זה\n",
      "    expected > אני מוכרח ללכת לישון .\n",
      "\n",
      "  minibatch loss: 10.020257949829102\n",
      "  minibatch loss: 9.932844161987305\n",
      "  minibatch loss: 9.843443870544434\n",
      "  minibatch loss: 9.708967208862305\n",
      "  minibatch loss: 9.598190307617188\n",
      "  sample 1:\n",
      "    input     > he is about thirty .\n",
      "    predicted_raw > [1 1 4 3 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    expected_raw > [  12.  926.  227.  857.    4.    3.    0.    0.    0.    0.    0.    0.\n",
      "    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "    0.    0.    0.    0.    0.]\n",
      "    predicted > <UNK> <UNK> .\n",
      "    expected > הוא בערך בן שלושים .\n",
      "  sample 2:\n",
      "    input     > he was in the army for thirty years .\n",
      "    predicted_raw > [1 1 1 4 4 4 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    expected_raw > [  1.20000000e+01   1.80000000e+01   8.48200000e+03   8.57000000e+02\n",
      "   3.66000000e+02   4.00000000e+00   3.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00]\n",
      "    predicted > <UNK> <UNK> <UNK> . . .\n",
      "    expected > הוא היה בצבא שלושים שנה .\n",
      "  sample 3:\n",
      "    input     > he stalled the engine three times .\n",
      "    predicted_raw > [1 1 1 4 4 3 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    expected_raw > [  1.20000000e+01   9.03000000e+02   1.52200000e+03   1.90480000e+04\n",
      "   2.35000000e+02   5.37000000e+02   4.00000000e+00   3.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00]\n",
      "    predicted > <UNK> <UNK> <UNK> . .\n",
      "    expected > הוא גרם למנוע להשתנק שלוש פעמים .\n",
      "\n",
      "  minibatch loss: 9.506037712097168\n",
      "  minibatch loss: 9.40553092956543\n",
      "  minibatch loss: 9.322525024414062\n",
      "  minibatch loss: 9.165563583374023\n"
     ]
    }
   ],
   "source": [
    "max_batches = 100001\n",
    "batches_per_epoch = int(len(vect_eng_sentences)/batch_size)\n",
    "log_every_iterations = 50\n",
    "print_samples_every_iterations = 250\n",
    "feed_generator = get_batch(batch_size)\n",
    "current_learning_rate = 0.1\n",
    "try:\n",
    "    for batch in range(max_batches):\n",
    "        if batch%batches_per_epoch==0:\n",
    "            print(\"Epoch %s\"%(int(batch/batches_per_epoch)))\n",
    "        fd = next(feed_generator)\n",
    "#         fd[learning_rate] = current_learning_rate\n",
    "        _, l = sess.run([train_op, loss], fd)\n",
    "#         if len(loss_track)>0 and loss_track[-1]<=l and current_learning_rate>0.002:\n",
    "#             current_learning_rate = current_learning_rate/2\n",
    "#             print(\"Learning rate dropped to %s\"%current_learning_rate)\n",
    "        loss_track.append(l)\n",
    "        if batch == 0 or batch % log_every_iterations == 0:\n",
    "#             print('batch {}'.format(batch))\n",
    "            print('  minibatch loss: {}'.format(l))\n",
    "            if batch % print_samples_every_iterations == 0:\n",
    "                predict_ = sess.run(decoder_prediction, fd)\n",
    "                for i, (inp, pred,exp) in enumerate(zip(fd[encoder_inputs].T, predict_.T,fd[decoder_targets].T)):\n",
    "                    print('  sample {}:'.format(i + 1))\n",
    "                    print('    input     > {}'.format(decode_sequence(inp[::-1],rev_eng_vocab)))\n",
    "                    print('    predicted_raw > {}'.format(pred))\n",
    "                    print('    expected_raw > {}'.format(exp))\n",
    "                    print('    predicted > {}'.format(decode_sequence(pred,rev_heb_vocab)))\n",
    "                    print('    expected > {}'.format(decode_sequence(exp,rev_heb_vocab)))\n",
    "                    if i >= 2:\n",
    "                        break\n",
    "                print()\n",
    "except KeyboardInterrupt:\n",
    "    print('training interrupted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
